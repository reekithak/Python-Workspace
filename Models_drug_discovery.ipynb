{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(r\"C:\\Users\\reekithak\\notebooks\\AzureML\\Drug_discovery Files\\drugs.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 65 graph sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating train_set lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_ = []\n",
    "for x in range(0,65):\n",
    "    xl_.append(\"train_set{}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(clf, x_t, y_t, x_v=None, y_v=None):\n",
    "    clf.fit(x_t, y_t)\n",
    "    print(\"Training Score\", clf.score(x_t, y_t))\n",
    "\n",
    "    if x_v is not None:\n",
    "        y_pred = clf.predict(x_v)\n",
    "        print(\"Validation Score\", clf.score(x_v, y_v))\n",
    "        print(\"F1 Score\", metrics.f1_score(y_pred, y_v, average=\"micro\"))\n",
    "        # print(\"ROC AUC Score\", metrics.roc_auc_score(y_pred, y_v))\n",
    "        print(\"Recall Score\", metrics.recall_score(y_pred, y_v, average=\"micro\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xl_ = [\"train_set6\",\"train_set7\",\"train_set8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"C:/Users/reekithak/notebooks/AzureML/Drug_discovery Files/Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Completed  train_set6\n",
      "Data Flow Completed\n",
      "train_set6\n",
      "Model Building Starting\n",
      "Model Building Done\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "Fetching Completed  train_set7\n",
      "Data Flow Completed\n",
      "train_set7\n",
      "Model Building Starting\n",
      "Model Building Done\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "Fetching Completed  train_set8\n",
      "Data Flow Completed\n",
      "train_set8\n",
      "Model Building Starting\n",
      "Model Building Done\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for set_ in xl_:\n",
    "    df = pd.read_sql(\"SELECT * from {}\".format(set_),conn)\n",
    "    cids = df['cid'].unique() \n",
    "    print(\"Fetching Completed\",\"\",set_)\n",
    "    unique_targets = set(\",\".join(df['target'].values.flatten().tolist()).split(\",\"))\n",
    "    #Creating data flow\n",
    "    id_dict = {v:k for k,v in dict(enumerate(unique_targets)).items()}\n",
    "    X = []\n",
    "    y = []\n",
    "    for cid in cids:\n",
    "        indxs = np.where(df['cid']==cid)[0]\n",
    "        X.append(df.iloc[indxs,1:-1].values)\n",
    "\n",
    "        yi = np.zeros(len(id_dict))\n",
    "        targets = \",\".join(df.iloc[indxs,-1].values.tolist()).split(\",\")\n",
    "        targets = [id_dict[i] for i in targets if i!=\"\"]\n",
    "        yi[targets] = 1\n",
    "        y.append(yi)\n",
    "    X = np.array(X).reshape(-1,32)\n",
    "    y = np.array(y)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    print(\"Data Flow Completed\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(set_)\n",
    "    path = os.path.join(main_dir,set_)\n",
    "    os.mkdir(path,mode=0o666)\n",
    "    if df.shape[0]<100000:\n",
    "        x_t , x_v , y_t , y_v = train_test_split(X,y,test_size = 0.1,random_state=42)\n",
    "    else:\n",
    "        x_t , x_v , y_t , y_v = train_test_split(X,y,test_size = 0.01,random_state=42)\n",
    "    print(\"Model Building Starting\")\n",
    "    \n",
    "    #Logistic regression\n",
    "    try:\n",
    "        \n",
    "        lgr = OneVsRestClassifier(LogisticRegression())\n",
    "        lgr = lgr.fit(x_t, y_t)\n",
    "        filename = \"/Regressor_model.sav\"\n",
    "        filename = path+ filename\n",
    "        pickle.dump(lgr,open(filename,'wb'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #SVC\n",
    "    try:\n",
    "        svc = OneVsRestClassifier(SVC())\n",
    "        svc = svc.fit(x_t,y_t)\n",
    "        filename = \"/SVC_model.sav\"\n",
    "        filename = path+ filename\n",
    "        pickle.dump(svc,open(filename,'wb'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #KNN\n",
    "    try:\n",
    "        \n",
    "        knn = OneVsRestClassifier(KNeighborsClassifier())\n",
    "        knn = knn.fit(x_t,y_t)\n",
    "        filename = \"/KNN_model.sav\"\n",
    "        filename = path+ filename\n",
    "        pickle.dump(knn,open(filename,'wb'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"Model Building Done\")\n",
    "    print(\"******************************************************************************************\")\n",
    "    print(\"******************************************************************************************\")\n",
    "    print(\"******************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\\nrf = fit_model(rf,x_t, y_t, x_v, y_v)\\nxgb = XGBClassifier()\\nxgb = fit_model(xgb,x_t, y_t, x_v, y_v)\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "rf = fit_model(rf,x_t, y_t, x_v, y_v)\n",
    "xgb = XGBClassifier()\n",
    "xgb = fit_model(xgb,x_t, y_t, x_v, y_v)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
